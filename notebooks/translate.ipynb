{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "sys.path.append(\"../\")\n",
    "import string\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from typing import Any\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSeq2SeqLM\n",
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse src_tgt training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/common_voice_nl/train.tsv\"\n",
    "df = pd.read_csv(filepath, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29031 [00:00<?, ?it/s]\r 11%|█▏        | 3267/29031 [00:00<00:00, 32659.91it/s]\r 23%|██▎       | 6623/29031 [00:00<00:00, 33181.14it/s]\r 34%|███▍      | 9942/29031 [00:00<00:00, 33124.52it/s]\r 46%|████▌     | 13255/29031 [00:00<00:00, 33029.50it/s]\r 57%|█████▋    | 16591/29031 [00:00<00:00, 33147.01it/s]\r 69%|██████▊   | 19945/29031 [00:00<00:00, 33278.00it/s]\r 80%|████████  | 23295/29031 [00:00<00:00, 33346.55it/s]\r 92%|█████████▏| 26630/29031 [00:00<00:00, 33153.70it/s]\r100%|██████████| 29031/29031 [00:00<00:00, 33112.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "      <th>stt_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382934.mp3</td>\n",
       "      <td>een daadwerkelijke keuzevrijheid voor ouderen ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>een daadwerkelijke keuzevrijheid voor ouderen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382935.mp3</td>\n",
       "      <td>elke kandidaatlidstaat moet op zijn eigen meri...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>elke kandidaat dit staat moet op zijn eigen wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382936.mp3</td>\n",
       "      <td>het verslag legt sterke nadruk op het nauwe ve...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>het verslag legt sterke nadruk op het nauwe ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382937.mp3</td>\n",
       "      <td>wij openen nu het algemeen debat</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we openen nu het algemeen debat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...</td>\n",
       "      <td>common_voice_nl_30382938.mp3</td>\n",
       "      <td>die fase is gebaseerd op de testcyclus van per...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nederlands Nederlands</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>die fase is gebaseerd op de test van personena...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           client_id  \\\n",
       "0  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "1  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "2  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "3  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "4  da4b6d09a23e8a83f83fec4e302a82c500d2821c4bb4d4...   \n",
       "\n",
       "                           path  \\\n",
       "0  common_voice_nl_30382934.mp3   \n",
       "1  common_voice_nl_30382935.mp3   \n",
       "2  common_voice_nl_30382936.mp3   \n",
       "3  common_voice_nl_30382937.mp3   \n",
       "4  common_voice_nl_30382938.mp3   \n",
       "\n",
       "                                            sentence  up_votes  down_votes  \\\n",
       "0  een daadwerkelijke keuzevrijheid voor ouderen ...         2           0   \n",
       "1  elke kandidaatlidstaat moet op zijn eigen meri...         2           0   \n",
       "2  het verslag legt sterke nadruk op het nauwe ve...         2           0   \n",
       "3                   wij openen nu het algemeen debat         4           0   \n",
       "4  die fase is gebaseerd op de testcyclus van per...         4           0   \n",
       "\n",
       "   age gender                accents locale  segment  \\\n",
       "0  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "1  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "2  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "3  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "4  NaN    NaN  Nederlands Nederlands     nl      NaN   \n",
       "\n",
       "                                             stt_out  \n",
       "0  een daadwerkelijke keuzevrijheid voor ouderen ...  \n",
       "1  elke kandidaat dit staat moet op zijn eigen wo...  \n",
       "2  het verslag legt sterke nadruk op het nauwe ve...  \n",
       "3                    we openen nu het algemeen debat  \n",
       "4  die fase is gebaseerd op de test van personena...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stt = []\n",
    "for fname in tqdm(df.path.values):\n",
    "    with open(f\"../data/common_voice_nl/cv_nl_stt/{fname}.json\", \"r+\") as f:\n",
    "        stt_out = json.load(f)\n",
    "        all_stt.append(stt_out['results']['channels'][0]['alternatives'][0]['transcript'])\n",
    "df['stt_out'] = pd.Series(all_stt)\n",
    "df['sentence'] = df.sentence.str.lower()\n",
    "df['sentence'] = df.sentence.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NMT based Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "noisy_seq = df.stt_out.iloc[:idx]\n",
    "clean_seq = df.sentence.iloc[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Marianseq_infer(src_seq):\n",
    "#     logger.info(f\"Initial (Noisy) Sequence = {src_seq}\")\n",
    "#     tokenizer_nl_en = MarianTokenizer.from_pretrained(\"../model/opus-mt-nl-en\")\n",
    "#     nl_en = MarianMTModel.from_pretrained(\"../model/opus-mt-nl-en\")\n",
    "    \n",
    "#     tokenizer_en_nl = MarianTokenizer.from_pretrained(\"../model/opus-mt-en-nl\")\n",
    "#     en_nl = MarianMTModel.from_pretrained(\"../model/opus-mt-en-nl\")\n",
    "\n",
    "#     encoder_batch = tokenizer_nl_en([src_seq], return_tensors=\"pt\")\n",
    "#     outputs_interim = nl_en.generate(**encoder_batch)\n",
    "#     interim_seq = tokenizer_nl_en.batch_decode(outputs_interim, skip_special_tokens=True)[0]\n",
    "    \n",
    "#     logger.info(f\"Interim Sequence  = {interim_seq}\")\n",
    "    \n",
    "#     decoder_batch = tokenizer_en_nl([interim_seq], return_tensors=\"pt\")\n",
    "#     outputs_final = en_nl.generate(**decoder_batch)\n",
    "#     tgt_seq = tokenizer_en_nl.batch_decode(outputs_final, skip_special_tokens=True)[0]\n",
    "    \n",
    "#     logger.info(f\"Final Sequence  = {tgt_seq}\")\n",
    "#     return tgt_seq\n",
    "# Marianseq_infer(noisy_seq)\n",
    "# logger.info(f\"Ground Truth Sequence = {clean_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_infer(src_batch):\n",
    "    logger.info(f\"Initial (Noisy) Sequence = {src_batch}\")\n",
    "    tokenizer_nl_en = AutoTokenizer.from_pretrained(\"../model/opus-mt-nl-en\")\n",
    "    nl_en = AutoModelForSeq2SeqLM.from_pretrained(\"../model/opus-mt-nl-en\").to(device)\n",
    "    \n",
    "    tokenizer_en_nl = AutoTokenizer.from_pretrained(\"../model/opus-mt-en-nl\")\n",
    "    en_nl = AutoModelForSeq2SeqLM.from_pretrained(\"../model/opus-mt-en-nl\").to(device)\n",
    "\n",
    "    encoder_input_ids = tokenizer_nl_en(src_batch, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "    outputs_interim = nl_en.generate(encoder_input_ids, num_beams=5, max_new_tokens=512)\n",
    "    interim_batch = tokenizer_nl_en.batch_decode(outputs_interim, skip_special_tokens=True)\n",
    "    \n",
    "    logger.info(f\"Interim Sequence  = {interim_batch}\")\n",
    "    \n",
    "    decoder_input_ids = tokenizer_en_nl(interim_batch, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "    outputs_final = en_nl.generate(decoder_input_ids, num_beams=5, max_new_tokens=512)\n",
    "    tgt_batch = tokenizer_en_nl.batch_decode(outputs_final, skip_special_tokens=True)\n",
    "\n",
    "    return tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_custom_generate(src_batch):\n",
    "    logger.info(f\"Initial (Noisy) Sequence = {src_batch}\")\n",
    "    tokenizer_nl_en = AutoTokenizer.from_pretrained(\"../model/opus-mt-nl-en\")\n",
    "    nl_en = AutoModelForSeq2SeqLM.from_pretrained(\"../model/opus-mt-nl-en\").to(device)\n",
    "    \n",
    "    tokenizer_en_nl = AutoTokenizer.from_pretrained(\"../model/opus-mt-en-nl\")\n",
    "    en_nl = AutoModelForSeq2SeqLM.from_pretrained(\"../model/opus-mt-en-nl\").to(device)\n",
    "\n",
    "    decoder_input_ids = tokenizer_nl_en(src_batch, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "    outputs_interim = nl_en(input_ids=decoder_input_ids)\n",
    "    logger.info(outputs_interim.size())\n",
    "    interim_batch = tokenizer_nl_en.batch_decode(outputs_interim, skip_special_tokens=True)\n",
    "    \n",
    "    logger.info(f\"Interim Sequence  = {interim_batch}\")\n",
    "    \n",
    "    decoder_input_ids = tokenizer_en_nl(interim_batch, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "    outputs_final = en_nl.generate(decoder_input_ids, num_beams=5, max_new_tokens=512)\n",
    "    tgt_batch = tokenizer_en_nl.batch_decode(outputs_final, skip_special_tokens=True)\n",
    "\n",
    "    return tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-27 23:04:18.100 | INFO     | __main__:seq2seq_custom_generate:2 - Initial (Noisy) Sequence = ['een daadwerkelijke keuzevrijheid voor ouderen daar zouden we werk van moeten maken', 'elke kandidaat dit staat moet op zijn eigen worden beoordeeld', 'het verslag legt sterke nadruk op het nauwe verband tussen de twee', 'we openen nu het algemeen debat', \"die fase is gebaseerd op de test van personenauto's\"]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either decoder_input_ids or decoder_inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tgt_batch \u001b[38;5;241m=\u001b[39m \u001b[43mseq2seq_custom_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Sequence  = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGround Truth Sequence = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_seq\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [12], line 10\u001b[0m, in \u001b[0;36mseq2seq_custom_generate\u001b[0;34m(src_batch)\u001b[0m\n\u001b[1;32m      7\u001b[0m en_nl \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../model/opus-mt-en-nl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m decoder_input_ids \u001b[38;5;241m=\u001b[39m tokenizer_nl_en(src_batch, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m outputs_interim \u001b[38;5;241m=\u001b[39m \u001b[43mnl_en\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(outputs_interim\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     12\u001b[0m interim_batch \u001b[38;5;241m=\u001b[39m tokenizer_nl_en\u001b[38;5;241m.\u001b[39mbatch_decode(outputs_interim, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Development/tinkering/stt_postprocess/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Development/tinkering/stt_postprocess/venv/lib/python3.9/site-packages/transformers/models/marian/modeling_marian.py:1440\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1435\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1436\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1437\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1438\u001b[0m         )\n\u001b[0;32m-> 1440\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1441\u001b[0m     input_ids,\n\u001b[1;32m   1442\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1443\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1444\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1445\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1446\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1447\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1448\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1449\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1450\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1451\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1452\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1453\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1454\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1455\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1456\u001b[0m )\n\u001b[1;32m   1457\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   1459\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/tinkering/stt_postprocess/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Development/tinkering/stt_postprocess/venv/lib/python3.9/site-packages/transformers/models/marian/modeling_marian.py:1240\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1233\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1234\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   1235\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1237\u001b[0m     )\n\u001b[1;32m   1239\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   1241\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1242\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1243\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   1244\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1245\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1246\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1247\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1248\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1249\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1250\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1251\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1252\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1253\u001b[0m )\n\u001b[1;32m   1255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1256\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/Development/tinkering/stt_postprocess/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Development/tinkering/stt_postprocess/venv/lib/python3.9/site-packages/transformers/models/marian/modeling_marian.py:968\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    966\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either decoder_input_ids or decoder_inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    970\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n\u001b[1;32m    971\u001b[0m past_key_values_length \u001b[39m=\u001b[39m past_key_values[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"
     ]
    }
   ],
   "source": [
    "tgt_batch = seq2seq_custom_generate(noisy_seq.values.tolist())\n",
    "logger.info(f\"Final Sequence  = {tgt_batch}\")\n",
    "logger.info(f\"Ground Truth Sequence = {clean_seq.values.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Custom Dataset and a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df) -> None:\n",
    "        super().__init__()\n",
    "        self.src = df.stt_out.values.tolist()\n",
    "        self.tgt = df.sentence.values.tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tgt)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_text = self.src[index]\n",
    "        target_text = self.tgt[index]\n",
    "        sample = {\"src\": source_text, \"tgt\": target_text}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):    \n",
    "     source = [x['src'].strip() for x in batch]\n",
    "     target = [x['tgt'].strip() for x in batch]\n",
    "     return source, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up Pytorch Lightning based Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.rand(5, 17, 67028)\n",
    "# b = torch.rand(5, 17)\n",
    "# a[-1, :].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Forward pass of en-nl implemented successfully. Can I also use forward pass of the encoder(nl-en)?\n",
    "- Dataloader: Pass tokenized sequences in batch instead of text\n",
    "- Implement WER metric tracking\n",
    "- Implement callbacks\n",
    "- Implement validation step and a custom generate function\n",
    "- Move all code to src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anand/Development/tinkering/stt_postprocess/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chained_seq2seq(LightningModule):\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer_encoder = AutoTokenizer.from_pretrained(\"../model/opus-mt-nl-en\")\n",
    "        self.encoder = AutoModelForSeq2SeqLM.from_pretrained(\"../model/opus-mt-nl-en\").to(device)\n",
    "        \n",
    "        self.tokenizer_decoder = AutoTokenizer.from_pretrained(\"../model/opus-mt-en-nl\")\n",
    "        self.decoder = AutoModelForSeq2SeqLM.from_pretrained(\"../model/opus-mt-en-nl\").to(device)\n",
    "        # output is logits. CE loss applies log_softmax to logits and then computes nll loss\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer_decoder.pad_token_id)\n",
    "    \n",
    "    def forward(self, inputs, tgt_ids):\n",
    "        encoder_input_ids = self.tokenizer_encoder(inputs, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        logger.info(f\"source batch size = {encoder_input_ids.size()}\")\n",
    "        outputs_interim = self.encoder.generate(encoder_input_ids, num_beams=5, max_new_tokens=512) \n",
    "        interim_batch = self.tokenizer_encoder.batch_decode(outputs_interim, skip_special_tokens=True)\n",
    "\n",
    "        decoder_input_ids = self.tokenizer_decoder(interim_batch, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        decoder_attention_masks = self.tokenizer_decoder(interim_batch, return_tensors=\"pt\", padding=True).attention_mask.to(device)\n",
    "        outputs_final = self.decoder(input_ids=decoder_input_ids, attention_mask = decoder_attention_masks, labels=tgt_ids)\n",
    "\n",
    "        return outputs_final\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src_batch, tgt_batch = batch\n",
    "        tgt_batch = self.tokenizer_encoder(tgt_batch, return_tensors=\"pt\",padding=True).input_ids.to(device)\n",
    "\n",
    "        outputs = self.forward(src_batch, tgt_batch)\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        logger.info(f\"target batch size = {tgt_batch.size()}\")\n",
    "        logger.info(f\"model output shape = {logits.size()}\")\n",
    "        logger.info(f\"returned loss = {loss}, calculated loss={self.criterion(logits[-1, :], tgt_batch[-1, :])}\")\n",
    "        self.log_dict({\n",
    "            'train_loss': loss\n",
    "        })\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ds = CustomDataset(df)\n",
    "        dl = DataLoader(\n",
    "            ds,\n",
    "            shuffle=True,  # False, if overfit_pct\n",
    "            batch_size=batch_size,\n",
    "            num_workers=12,\n",
    "            collate_fn=collate_batch)\n",
    "        print(\n",
    "            f\"dataset:'{'train'}', size:{len(ds)}, batch:{batch_size}, nb_batches:{len(dl)}\"\n",
    "        )\n",
    "        return dl\n",
    "\n",
    "    def generate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "model = chained_seq2seq()\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator=\"auto\", callbacks=[TQDMProgressBar(refresh_rate=10)], devices=1 if torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | encoder   | MarianMTModel    | 79.0 M\n",
      "1 | decoder   | MarianMTModel    | 79.0 M\n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "156 M     Trainable params\n",
      "1.0 M     Non-trainable params\n",
      "157 M     Total params\n",
      "631.849   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:'train', size:29031, batch:16, nb_batches:1815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3fc5a16f9841c295b0e2b859acfd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 00:52:41.124 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 15])\n",
      "2022-10-26 00:52:43.128 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 17])\n",
      "2022-10-26 00:52:43.128 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 17, 67028])\n",
      "2022-10-26 00:52:43.130 | INFO     | __main__:training_step:33 - returned loss = 4.460850238800049, calculated loss=3.1101460456848145\n",
      "2022-10-26 00:52:44.033 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 24])\n",
      "2022-10-26 00:52:46.407 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 14])\n",
      "2022-10-26 00:52:46.408 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 14, 67028])\n",
      "2022-10-26 00:52:46.410 | INFO     | __main__:training_step:33 - returned loss = 25.047887802124023, calculated loss=29.470069885253906\n",
      "2022-10-26 00:52:47.191 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 16])\n",
      "2022-10-26 00:52:49.092 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 16])\n",
      "2022-10-26 00:52:49.093 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 16, 67028])\n",
      "2022-10-26 00:52:49.094 | INFO     | __main__:training_step:33 - returned loss = 21.908254623413086, calculated loss=16.022552490234375\n",
      "2022-10-26 00:52:49.863 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 25])\n",
      "2022-10-26 00:52:52.219 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 17])\n",
      "2022-10-26 00:52:52.220 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 17, 67028])\n",
      "2022-10-26 00:52:52.222 | INFO     | __main__:training_step:33 - returned loss = 10.234868049621582, calculated loss=10.812248229980469\n",
      "2022-10-26 00:52:53.005 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 28])\n",
      "2022-10-26 00:52:55.844 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 14])\n",
      "2022-10-26 00:52:55.845 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 14, 67028])\n",
      "2022-10-26 00:52:55.846 | INFO     | __main__:training_step:33 - returned loss = 8.91227912902832, calculated loss=10.7593355178833\n",
      "2022-10-26 00:52:56.650 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 16])\n",
      "2022-10-26 00:52:58.468 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 20])\n",
      "2022-10-26 00:52:58.468 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 20, 67028])\n",
      "2022-10-26 00:52:58.470 | INFO     | __main__:training_step:33 - returned loss = 8.960888862609863, calculated loss=7.441479206085205\n",
      "2022-10-26 00:52:59.281 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 16])\n",
      "2022-10-26 00:53:01.128 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 17])\n",
      "2022-10-26 00:53:01.128 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 17, 67028])\n",
      "2022-10-26 00:53:01.130 | INFO     | __main__:training_step:33 - returned loss = 8.104205131530762, calculated loss=8.739314079284668\n",
      "2022-10-26 00:53:01.913 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 18])\n",
      "2022-10-26 00:53:03.588 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 18])\n",
      "2022-10-26 00:53:03.589 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 18, 67028])\n",
      "2022-10-26 00:53:03.591 | INFO     | __main__:training_step:33 - returned loss = 5.977665424346924, calculated loss=9.681803703308105\n",
      "2022-10-26 00:53:04.367 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 16])\n",
      "2022-10-26 00:53:08.012 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 15])\n",
      "2022-10-26 00:53:08.012 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 15, 67028])\n",
      "2022-10-26 00:53:08.014 | INFO     | __main__:training_step:33 - returned loss = 6.8376922607421875, calculated loss=8.890703201293945\n",
      "2022-10-26 00:53:08.859 | INFO     | __main__:forward:14 - source batch size = torch.Size([16, 28])\n",
      "2022-10-26 00:53:11.593 | INFO     | __main__:training_step:31 - target batch size = torch.Size([16, 16])\n",
      "2022-10-26 00:53:11.594 | INFO     | __main__:training_step:32 - model output shape = torch.Size([16, 16, 67028])\n",
      "2022-10-26 00:53:11.597 | INFO     | __main__:training_step:33 - returned loss = 6.316880226135254, calculated loss=10.333537101745605\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = chained_seq2seq().eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../model/opus-mt-en-nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['een echte keuzevrijheid voor ouderen.',\n",
       " 'elke kandidaat deze staat moet worden beoordeeld op zijn eigen',\n",
       " 'In het verslag wordt sterk de nadruk gelegd op het nauwe verband tussen de twee',\n",
       " 'Wij openen nu het algemene debat',\n",
       " \"die fase is gebaseerd op de test van personenauto's\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model(noisy_seq.values.tolist()), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a3b7c7159b609b6d6ed4cb8640074f3b1e9bee62f57677af0169d92fbf130be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
